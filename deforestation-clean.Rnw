\documentclass[10pt,english]{article}
\usepackage[paperwidth=164mm, paperheight=280mm, top=19mm, bottom=19mm, left=1cm, right=1cm]{geometry} % 16:9 three pages in a row
\usepackage[english]{babel}
\usepackage{longtable}
\usepackage{fontspec}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{varioref} %% vref
\usepackage{paralist} %% compactenum
\usepackage{siunitx} %% \num
\usepackage{IEEEtrantools} % proper indentation for multi-line equations
\usepackage{amsmath} %% \pmatrix
\usepackage{subfig}

%% make a single « start and end a chunk of \texttt{}, and two «« to represent the literal «.
\catcode`«=\active
\def«#1«{\texttt{#1}}

% This chunk is needed to make sure evaluation is taking place in the current directory.
<<setup.outer, echo=FALSE, tidy=FALSE, message=FALSE>>=
knitr::opts_chunk$set(results='hide', cache=TRUE, echo=TRUE, warning=TRUE, fig.pos = 'htb', tidy.opts=list(blank=FALSE, width.cutoff=50), background='white', tidy=FALSE, error=TRUE, message=FALSE, dev="pdf", autodep = TRUE)
knitr::opts_knit$set(root.dir = ".")
knitr::knit_hooks$set(crop = knitr::hook_pdfcrop)
options(scipen=999) ## Avoid scientific formating of large numbers.
@

\begin{document}

<<setup.more, echo = FALSE>>=
my.dir <- "global-living-conditions"

## If you have a local cache of files downloaded from the DHS website
## then change my.dir to point to that folder

if(Sys.info()["sysname"] == "Windows"){
  ## figure out which computer we are in. TODO consider make all paths relative to the working directory.
  if(file.exists("D:/Hans/DHS/global-living-conditions")){
      my.dir <- "D:/Hans/DHS/global-living-conditions"    
  } else {
      if(file.exists("F:/Hans/global-living-conditions")){
          my.dir <- "F:/Hans/global-living-conditions"
    } else {
      stop("no data directory found in two spots for Windows.")
    }
  }
} else {
    ## 245
    if(file.exists("/media/sf_Our_data/Data/DHS")){
        my.dir <- "/media/sf_Our_data/Data/DHS"
        deforestation.path <- "/media/sf_Our_data/Hans/Onedrive/OneDrive - University of Gothenburg/Data/Global_forest_coverage/"
        africa.map.path <- "~/annex/projekt/child-poverty/statiska-eller-stora-filer/"
    }
    ## SNIC
    if(file.exists("/mimer/NOBACKUP/groups/globalpoverty1/hans")){
        my.dir <- "/mimer/NOBACKUP/groups/globalpoverty1/hans/global-living-conditions"
        country.data.path <- "/mimer/NOBACKUP/groups/globalpoverty1/hans/Deforestation/Country data/"
        deforestation.path <- "/mimer/NOBACKUP/groups/globalpoverty1/hans/Deforestation/"
        africa.map.path <- "~/maps/"
    }
    ## nomad, only do this for a small set
    if(file.exists("~/mnt/poverty/media/sf_Our_data/Data/DHS")){
        my.dir <- "~/mnt/poverty/media/sf_Our_data/Data/DHS"
        deforestation.path <- "~/mnt/poverty/media/sf_Our_data/Hans/Onedrive/OneDrive - University of Gothenburg/Data/Global_forest_coverage/"
        africa.map.path <- "~/annex/projekt/child-poverty/statiska-eller-stora-filer/"
        country.data.path <- "~/annex/projekt/child-poverty/statiska-eller-stora-filer/deforestation/Country data/"
    }
}
@

<<config>>=
my.countries <- c("Senegal", "Gambia", "Guinea", "Sierra Leone", "Liberia", "Mali", "Cote d'Ivoire", "Burkina Faso", "Ghana", "Benin", "Togo", "Nigeria", "Chad", "Cameroon", "Sudan", "Ethiopia", "Sudan", "Gabon", "Congo", "Congo Democratic Republic", "Uganda", "Rwanda", "Kenya", "Tanzania", "Angola", "Malawi", "Mozambique", "Zambia", "Zimbabwe", "Namibia", "Lesotho", "South Africa", "Eswatini", "Madagascar", "Egypt", "Tunisia", "Morocco", "Burundi")

my.countries <- c("Tanzania")

if(!exists("my.countries")){
    my.countries = NULL
}

if(!exists("my.waves")){
    my.waves = NULL
}

@

<<install.packages, cache = FALSE>>=
required.packages <- c("httr", "data.table", "car", "sp", "raster", "devtools", "purrr", "doParallel", "snow", "viridisLite", "geosphere", "parallel", "foreach", "tmap", "terra", "sf", "lme4", "parallelly", "readxl")
for(package in required.packages){
    if(!require(package, character.only=TRUE)){
        install.packages(package, character.only=TRUE)
    }
}
library(devtools)
devtools::install_github("matthewkling/colors3d")
@


Create a custom subset for each deprivation, including only the cases which have data on this deprivation and add country level factors.

<<continued.country>>=
library("readxl"); library(data.table)
country.data <- Reduce(merge, sapply(c("API_NY.GDP.PCAP.CD_DS2_en_excel_v2_5795829.xlsx", "API_NE.EXP.GNFS.ZS_DS2_en_excel_v2_5728793.xlsx", "API_NV.AGR.TOTL.ZS_DS2_en_excel_v2_5728821.xlsx", "API_SP.POP.1564.TO_DS2_en_excel_v2_5731200.xlsx"), function(x) {
    temp.dt.1 <- data.table(read_excel(paste0(country.data.path, x), skip = 2))
    meta.data <- temp.dt.1[["Indicator Name"]][1]
    temp.dt.1[, c("Indicator Name", "Indicator Code", "Country Code") := NULL, ]
    temp.dt.2 <- melt(temp.dt.1, id.vars = "Country Name", variable.name = "raw.year", value.name = meta.data)
    temp.dt.2$raw.year <- as.numeric(as.character(temp.dt.2$raw.year))
    colnames(temp.dt.2) <- gsub("Country Name", "country", colnames(temp.dt.2))
    return(temp.dt.2)
}, simplify = FALSE))
## improve some var names
bad.names <- c("GDP per capita (current US$)", "Exports of goods and services (% of GDP)", "Agriculture, forestry, and fishing, value added (% of GDP)", "Population ages 15-64, total")
nice.names <- c("GDP", "percent.goods.and.services.percent.of.gdp", "agriculture.forestry.percent.of.gdp", "population.15.64")
for(i in 1:length(bad.names)){
    colnames(country.data) <- gsub(bad.names[i], nice.names[i], colnames(country.data), fixed = TRUE)
}
@

Each subset includes the average education in years for the adults aged 24-44 in year 2000 and aged 45-65 in year 2021, in order for us to be able to estimate how different the group surveyed in one year is from the group surveyed another year (in the same area), even if the educational level is increasing during the study period. By focussing on the persons who have already aquired their education when the study starts, we make use of the invariant feature of education, it is never lost.


<<load.data>>=
library(devtools)
install_bitbucket(repo = "hansekbrand/iwi") ## install iwi first, since DHSharmonisation depends on it.
install_bitbucket(repo = "hansekbrand/DHSharmonisation", ref="debug")
library(globallivingconditions)
load("~/user_credentials_dhs.RData") # a list with two named elements:
                                   # "dhs.user" and "dhs.password" 
my.dt <- download.and.harmonise(
    dhs.user=credentials$dhs.user,
    dhs.password=credentials$dhs.password,
    countries = my.countries,
    waves = my.waves,
    vars.to.keep=c("source", "version", "year.of.interview", "country.code.ISO.3166.alpha.3", 
"m49.region", "RegionID", "district", "ClusterID", "ClusterID.clean", 
"PersonID.unique", "severe.food.deprivation", "severe.water.deprivation", 
"severe.sanitation.deprivation", "severe.shelter.deprivation", "has.electricity",
"HouseholdID", "month.of.interview", "sample.weight", "water", "time.to.water", "iwi",
"rural", "lon", "lat", "phone", "owns.tv", "owns.land", "refridgerator", "owns.livestock",
"gadm.areaname.3", "gadm.areaname.2", "gadm.areaname.1",
"relation.to.hh", "age", "education.in.years", "education", "superClusterID"),
    check.dhs.for.more.data = TRUE,
    variable.packages = c("wealth", "india", "empowerment"),
    required.number.of.waves = 2,
    file.types.to.download = c("PR", "GE", "IR", "MR", "KR"),
    directory = my.dir,
    make.pdf = FALSE
)
save(my.dt, file = "living-conditions.RData")

## Create the set of numeric codes of the countries in the sample.
country.codes <- my.dt[, unique(country.code.ISO.3166.alpha.3), ]
n.measurement.locations <- nrow(unique(my.dt[, list(lon, lat), ]))
n.persons <- nrow(my.dt)

rm(my.dt)
gc(verbose=FALSE)

@

To identify which grid cell(s) of deforestation data cover the gadm region, find the North West corner, that is, «max(latitude)« and «min(longitude)« of the bounding box, and that gives the first needed cell. Then extend to east and south if necessary. Assume no region covers more than four grid cells.

<<find.the.grid.cell>>=
library(httr)
base.url <- "https://storage.googleapis.com/earthenginepartners-hansen/GFC-2021-v1.9/"

## helper functions
url.from.point.f <- function(lon, lat, layer){
    lon.first.corner <- floor(lon / 10) * 10
    lat.first.corner <- ceiling(lat / 10) * 10
    if(abs(lon.first.corner) > 100){
        lon.corner <- paste0("0", abs(lon.first.corner))
    } else {
        ## This does not make sense, but it works.
        lon.corner <- paste0("0", abs(lon.first.corner))        
        ## lon.corner <- abs(lon.first.corner)
    }
    if(abs(lon.first.corner) < 10){
        ## longitude 0 is written 000E
        lon.corner <- paste0("00", abs(lon.first.corner))
    }    
    if(lon.first.corner < 0){
        lon.corner <- paste0(lon.corner, "W")
    } else {
        lon.corner <- paste0(lon.corner, "E")    
    }
    if(abs(lat.first.corner) < 10){
        ## latitude 0 is written 00N
        lat.corner <- paste0("0", abs(lat.first.corner))
    } else {
        lat.corner <- abs(lat.first.corner)        
    }
    if(lat.first.corner < 0){
        lat.corner <- paste0(lat.corner, "S")
    } else {
        lat.corner <- paste0(lat.corner, "N")    
    }
    my.file <- paste0("Hansen_GFC-2021-v1.9_", layer, "_", lat.corner, "_", lon.corner, ".tif")    
    return(my.file)
}
point.at.corners.f <- function(combination, my.bbox){
    c(my.bbox["x", combination[1]], my.bbox["y", combination[2]])
}

file.name.from.bbox.f <- function(bbox, layer){
    my.combinations <- unlist(sapply(1:2, function(x){ sapply(1:2, function(y) { c(x, y) }, simplify = FALSE) }, simplify = FALSE), recursive = FALSE)
    points.at.corners <- sapply(my.combinations, point.at.corners.f, my.bbox = bbox)
    urls.at.corners <- apply(points.at.corners, 2, function(x) { url.from.point.f(lon=x[1], lat=x[2], layer) })
    return(unique(urls.at.corners))
}

url.from.file.f <- function(file.name) { paste0(base.url, file.name) }

dhs.grid <- get(load("dhs.grid.RData"))

for(i in 1:length(dhs.grid)){
    my.bbox <- dhs.grid[i]@bbox
    for(my.layer in c("treecover2000", "lossyear")){
        my.files <- file.name.from.bbox.f(my.bbox, layer = my.layer)
        my.urls <- sapply(my.files, url.from.file.f)
        ## fetch missing files
        for(j in 1:length(my.files)){
            file.and.path <- paste0(deforestation.path, my.files[j])
            if(!file.exists(file.and.path)){
	        globallivingconditions::my.log.f(paste(Sys.time(), "country number", i, "j", j, "will download this url:", my.urls[j], "and save it as", file.and.path), "deforestation.log")
                httr::GET(my.urls[j], httr::write_disk(file.and.path))
            }
        }
    }
}
@


To be able to inspect single countries, write a function that loads the borders of the country.

<<borders>>=
get.border <- function(country.code) {
  string.code <- globallivingconditions::iso.3166$String.code[
    match(country.code, globallivingconditions::iso.3166$numeric)
  ]
  filename <- file.path(my.dir, "GIS-borders", paste0(string.code, "_adm0.rds"))
  if (file.exists(filename)) {
    return(sf::st_as_sf(readRDS(filename)))  # Convert from sp to sf
  } else {
    return(NULL)
  }
}

@ 

To get the country code from the name use «iso.3166« from «globallivingconditions«.

<<code.from.name>>=
code.from.name <- function(name) {
    globallivingconditions::iso.3166$numeric[match(name,
                    globallivingconditions::iso.3166$String)]
}
@ 

A polygon that covers the whole continent of interest, including countries on the contintent not included in the analysis, the object «africa« is a «terra« vector object, but old code furter down assumes it is a «SpatialPolygons« object, so convert into that.

% «SpatialPolygons« object. Use «raster::union()« to merge polygons to a single border.

<<the-whole-continent>>=
africa <- get(load(paste0(africa.map.path, "africa-lowres.RData")))
## The file originally contained africa.mainland and madagascar, both in lowres
## if(exists("africa") == FALSE){
##     africa <- raster::union(africa.mainland, madagascar)
##     rm(africa.mainland, madagascar); gc(verbose = FALSE)
## }
if(class(africa) == "PackedSpatVector"){
    africa <- terra::vect(africa)
}
library(raster)
## Convert back to sp
africa <- as(africa, "Spatial")
@ 

Create the surface covered by all countries in the sample, here called «my.window«.

<<create.the.window>>=
my.window <- country.codes |>
  lapply(get.border) |>
  (\(x) Filter(Negate(is.null), x))() |>
  (\(x) do.call(rbind, x))() |>
  sf::st_union() |>
  sf::as_Spatial()

@

<<plot-the-grid, fig.cap = paste0("The administrative areas at which we calculate the level of forestation and the level of deforestation. There are ", length(dhs.grid), " administrative areas. The red line encloses the area under study, ie. white areas without black lines in them are excluded. In total the data on living conditions consists of measurements from ", n.measurement.locations, " locations . Total number of surveyed persons: ", n.persons, "."), fig.width=6.45, fig.height=6.45>>=
plot(africa)
plot(dhs.grid, add = TRUE, lwd = 0.05)
plot(my.window, add = TRUE, border = "red", lwd = 0.5)
@ 

To be able to plot the whole continent, create a downsampled dataset. Downsample with «aggregate()« which by default takes the mean. A factor of 100 in each dimension gives a reduction of data by 10.000/1 in two dimensions.

<<downsampling>>=
cl <- snow::makeCluster(parallelly::availableCores(method = c("Slurm", "system"))-1, type = "SOCK")
doParallel::registerDoParallel(cl)
snow::clusterExport(cl, c("deforestation.path"))

foreach::foreach(file = list.files(path=deforestation.path, pattern = ".*treecover.*[W|E].tif", full.names=TRUE)) %dopar% {
    new.filename <- gsub(".tif", "_small.tif", file, fixed=TRUE)
    if(! file.exists(new.filename)){
        temp.raster <- raster::aggregate(raster::raster(file), 100)
        raster::writeRaster(temp.raster, filename = new.filename,
                    datatype='INT1U')
    }
}

stopCluster(cl)

@ 

<<plot-the-coverage, fig.cap = "The tree coverage in the sampled area year 2000 per pixel, ie any pixel can have any color. The raw data was downsampled 10.000 times before producing this plot.", fig.width=6.45, fig.height=6.45, dpi=300>>=
## terra requires my.window to be a spatvector, but that is inconvenient since is to not cacheable by knitr.
## So, make my.window a SpatialPolygons, and use raster:: instead of terra::
## plot(terra::mask(Reduce(terra::merge, sapply(list.files(path=deforestation.path, pattern = ".*treecover.*small.tif", full.names=TRUE), terra::rast)), my.window))
plot(raster::mask(Reduce(raster::merge, sapply(list.files(path=deforestation.path, pattern = ".*treecover.*small.tif", full.names=TRUE), raster::raster)), my.window))
lines(africa)
lines(my.window, col = "red", lwd = 0.5)
@ 

For verifying that the program is correct, the possibility to compare the pixel based version with the region based version of the same country is vital. Here is how to plot the pixel based version of a single country.

«crop()« reduces the extent so that it matches the bounding box of the region specified as the second argument to «crop()«. This means that «crop()« could just as well work with the bounding box of its second argument. In contrast «mask()« uses the full geometry of its second argument, and for each pixel that lie outside the border of this polygon, «mask()« sets its value to NA. In short, both «crop()« and «mask()« are needed to achieve a small (crop) and correctly encoded (mask) result.

\section{Explanation of the algorithm to produce the loss of tree coverage in an area}
Since the data is of very high resolution, and the total area of the window is large, this is a Big Data problem, the data to process does not fit into the RAM of the computer. On the one hand there are a set of data files, corresponding to rectangular patches of the earth of size 10 degree by 10 degree. On the other hand, there is the grid with \Sexpr{length(dhs.grid)} areas.  When an area covers multiple data files the conceptually easiest way to calculate a mean would be to use first use the function «crop()« to cut out the parts of the data files covered by the area at hand, and then use the function «merge()« to stitch these together to a single area and then proceed as usual, applying the functions «values()« and «mean()« to get the desired result. This process works only as long as the areas to be stitched together are small. To be able to process large areas a more complex algorithm is required, an algorithm that produces statistics for each part separately and then uses the parts to calculate a weighted mean. This is what «temporal.loss.directly.from.possible.multiple.files()« and its companion «average.coverage.over.time.directly.from.single.file()« accomplished (the word ``directly'' refers to that the algorithm does this without merging the partial areas to a single large area first).

To efficiently use these, the size of each area is computed, and all areas are classified by their size and each size-class is treated differently: the larger the areas in the size class, the more agressive parallelisation is applied (the largest areas are handled sequentially). In addition to this, parts of «average.coverage.over.time.directly.from.single.file()« is also parallelised in a way that takes the size of the partial area into account, but only on operating systems that support forking of processes, ie. Linux and MacOS.

Calculating the loss of tree coverage over time requires use of both the coverage layer and the loss layer, since the loss data is a binary flagg per pixel, to quantify the loss we need to know the state of that pixel before the loss happened (and as a proxy we must use the state of that pixel in year 2000). Fortunately, only the cells where loss occured is needed, which generally is a very small portion of all cells in the loss layer.


% \section{A package for accessing the global forest watch database}
% Ideally, there is a function that takes as input a polygon, some layers, and some function to derive some statistics on those layers. You run that function on all your polygons, and call it a day.

% The package must support a local cache so that it does not download any file more than once.

% I want temporal loss, that is for a given set of years, provide the percentage of loss for the polygon.
% Also, for the year 2000, what was the precentage covered by forest in the polygon.

% Does the former require the latter? Yes. the layer ``lossyear'' only gives loss as a percentage of the whole area, but we want that as a fraction of the percentage of the whole area \emph{that was covered by forest at t0}.

% But the former does not require the latter in the sense that both has to be in RAM simultaneously, and the calculation including both of them is done only on a scalar for each of them. This makes it sensible to have two independent funcctions.

% The latter function is easiest, lets start with that one.

% input a polygon
% output the average coverage rate of that polygon.

% This requires functionality to download, and load into RAM, the appropriate tiff file(s) and take a mean of the values.

% The core functionality here is really the first part, taking the mean is simple enough.

% Start with a helper function that gets the data into RAM.

<<get.raster>>=
## make sure the files are available locally
fetch.raster <- function(my.files, deforestation.path, verbose = FALSE) {
    ## download files, if they are missing
    my.urls <- sapply(my.files, url.from.file.f)    
    for(j in 1:length(my.files)){
        file.and.path <- paste0(deforestation.path, my.files[j])
        if(!file.exists(file.and.path)){
            globallivingconditions::my.log.f(paste(Sys.time(), "will download this url:", my.urls[j], "and save it as", file.and.path), "deforestation.log")
            httr::GET(my.urls[j], httr::write_disk(file.and.path))
        } else {
            if(verbose) {
                globallivingconditions::my.log.f(paste(Sys.time(), "File", my.files[j], "already downloaded to", file.and.path), "deforestation.log")
            }
        }
    }
}

raster.from.file.f <- function(file, my.mask, deforestation.path){
    r <- raster::raster(paste0(deforestation.path, file))
    r.cropped <- raster::crop(r, my.mask)
    r.masked <- raster::mask(r.cropped, my.mask)
    rm(r, r.cropped)
    return(r.masked)
}

load.raster.from.disk <- function(polygon, my.layer = "treecover2000", deforestation.path, ...){
    ## "lossyear" is the other layer of interest
    my.files <- file.name.from.bbox.f(sp::bbox(polygon), layer = my.layer)
    fetch.raster(my.files, deforestation.path = deforestation.path, ...)
    if(length(my.files) > 1){
        my.raster.list <- sapply(my.files, raster.from.file.f, my.mask = polygon, deforestation.path = deforestation.path, ...)
        my.raster <- Reduce(raster::merge, my.raster.list)
    } else {
        my.raster <- raster.from.file.f(my.files, my.mask = polygon, deforestation.path = deforestation.path, ...)
    }
    return(my.raster)
}

average.coverage.over.time.directly.from.single.file <- function(polygon, my.cover.file, my.loss.file, deforestation.path, ...){
    ## calculate size,  number of lost cells, per year.
    ## depends on raster.from.file.f()

    my.years = 1:21

    ## my.loss.raster <- raster.from.file.f(file = my.loss.file, my.mask = polygon, ...)
    my.loss.raster <- raster.from.file.f(file = my.loss.file, my.mask = polygon, deforestation.path = deforestation.path)
    loss <- raster::values(my.loss.raster)
    these.are.within.the.area <- which(is.na(loss) == FALSE)
    size <- length(these.are.within.the.area)
    these.were.lost.indices <- which(loss > 0)
    these.were.lost.values <- loss[these.were.lost.indices]
    rm(loss, my.loss.raster); trash <- gc(verbose = FALSE)
    raster::removeTmpFiles(h=0)

    ## my.cover.raster <- raster.from.file.f(file = my.cover.file, my.mask = polygon, ...)
    my.cover.raster <- raster.from.file.f(file = my.cover.file, my.mask = polygon, deforestation.path = deforestation.path)
    coverage <- raster::values(my.cover.raster)[these.are.within.the.area]
    rm(my.cover.raster); trash <- gc(verbose = FALSE)
    raster::removeTmpFiles(h=0)
    average.coverage.at.t0 <- mean(coverage)    
    ## The strategy is to calculate a new average for each year.
    ## And the final measure is the difference in these averages.

    my.f <- function(year) {
        these.cells.lost.this.year.or.earlier <- these.were.lost.indices[which(these.were.lost.values <= year)]
        ## calculate the new mean by adding zeros for all the lost cells, and removing the elements at those indices.
        if(length(these.cells.lost.this.year.or.earlier) > 0){
            ## since these.cells.lost.this.year.or.earlier are relative to these.were.lost,
            ## don't forget to include these.were.lost at next row.
            return(mean(c(coverage[-these.cells.lost.this.year.or.earlier],
                          rep(0, length(these.cells.lost.this.year.or.earlier))), na.rm=TRUE))
            ## <- na.rm=TRUE to handle pixels outside the region.
        } else {
            ## No loss has occured yet.
            return(average.coverage.at.t0)
        }
      }

    ## Skip this inner parallelisation now that that outer parellisation works
    ## parallelise if on unix
    ## if(.Platform$OS.type == "unix") {    
    if(FALSE) {
        n.cores <- floor((parallelly::availableCores(method = c("Slurm", "system"))-1) * min(1, 3E8/size))
        globallivingconditions::my.log.f(paste(Sys.time(), "Using", n.cores, "to calculate the yearly loss for an area of size:", size), "deforestation.log")    
        average.coverage.per.year <- parallel::mclapply(my.years, my.f, mc.cores = n.cores)
    } else {
        average.coverage.per.year <- sapply(my.years, my.f)
    }
    names(average.coverage.per.year) <- paste0("t", my.years)
    globallivingconditions::my.log.f(paste(Sys.time(), "Ready with temporal loss for an area of size", size, "for a polygon of size ",
                                           geosphere::areaPolygon(polygon)), "deforestation.log")    
    rm(polygon, my.cover.file, my.loss.file, coverage); gc(verbose = FALSE)
    raster::removeTmpFiles(h=0)
    return(list(size = size,
                coverage = c(t0 = average.coverage.at.t0, average.coverage.per.year)
                ))
}

temporal.loss.directly.from.possible.multiple.files <- function(index, deforestation.path, ...){
    ## index refers to dhs.grid
    polygon <- dhs.grid[index]
    my.bbox <- sp::bbox(polygon)
    my.cover.files <- file.name.from.bbox.f(bbox = my.bbox, layer = "treecover2000")
    my.loss.files <- file.name.from.bbox.f(bbox = my.bbox, layer = "lossyear")    
    fetch.raster(c(my.cover.files, my.cover.files), deforestation.path = deforestation.path, ...)
    if(length(my.cover.files) > 1){
        my.raster.stats <- sapply(1:length(my.cover.files), function (i) {
            average.coverage.over.time.directly.from.single.file(polygon = polygon, my.cover.file = my.cover.files[i], my.loss.file = my.loss.files[i], deforestation.path = deforestation.path)
        }, simplify = FALSE)
        ## If any element is NULL remove it
        these.are.null <- which(sapply(my.raster.stats, is.null))
        if(length(these.are.null) > 0) {
            ## assume not all of them are null
            my.raster.stats <- my.raster.stats[-these.are.null]
        }
        ## create a weighted mean
        weights <- as.vector(prop.table(unlist(sapply(seq(from = 1, to = length(my.raster.stats)), function(i) { my.raster.stats[[i]][1] }))))
        ## restructure so that we don't need to know the number of tiff-files were used
        my.data <- Reduce(cbind, sapply(1:length(my.raster.stats), function(i) { my.raster.stats[[i]][[2]] }, simplify = FALSE))
        weighted.means <- sapply(1:nrow(my.data), function(i) {
            round(weighted.mean(x = unlist(my.data[i, ]), w = weights), 16)
        })
        names(weighted.means) <- paste0("t", 0:(length(weighted.means)-1))
        my.raster.stats <- list(size = sum(unlist(sapply(seq(from = 1, to = length(my.raster.stats)), function(i) { my.raster.stats[[i]][1] }))),
                                coverage = weighted.means)
    } else {
        my.raster.stats <- average.coverage.over.time.directly.from.single.file(polygon = polygon, my.cover.file = my.cover.files, my.loss.file = my.loss.files, ...)
    }
    absolute.loss.per.year <- sapply(1:(length(my.raster.stats$coverage)-1), function(i) { my.raster.stats$coverage[["t0"]] - my.raster.stats$coverage[[paste0("t", i)]] })
    temporal.loss <- cbind(data.table::data.table(region = names(dhs.grid)[index],
                                                  size = geosphere::areaPolygon(polygon),
                                                  coverage.at.t0 = my.raster.stats$coverage[["t0"]]), t(data.frame(absolute.loss.per.year)))
    globallivingconditions::my.log.f(paste(Sys.time(), "Ready with temporal loss for region", names(dhs.grid)[index]), "deforestation.log")
    return(temporal.loss)
}
@ 

Try it out by plotting the first region in the grid, see figure \vref{fig:test-get-raster}.

<<test-get-raster, fig.cap = "Testing the functionality to get a raster given a polygon.", fig.width = 6, fig.height = 4>>=
library(raster)
plot(load.raster.from.disk(dhs.grid[1], deforestation.path = deforestation.path), zlim = c(0, 100))
plot(dhs.grid[1], add = TRUE)
@ 

Taking the mean value is easy enough. To see that the mean carries some signal, plot the mean values of all regions in the first country in the grid, and compare that with the raw version. First plot the raw data, see figure \vref{fig:raw-data-first-country}.

<<raw-data-first-country, fig.cap = "Raw data downsample by factor 100.", fig.width = 7, fig.height = 5.5>>=
these.regions <- which(substring(names(dhs.grid), 1, 4) == substring(names(dhs.grid)[1], 1, 4))
this.country.code <- substring(names(dhs.grid)[1], 1, 3)
this.country.name <- globallivingconditions::iso.3166$String[match(this.country.code, globallivingconditions::iso.3166$numeric)]
my.f <- function(x, y) { raster::merge(x, y, tolerance = 1) }
my.g <- function(i) {
    my.raster <- load.raster.from.disk(dhs.grid[i], deforestation.path = deforestation.path)
    raster::aggregate(my.raster, 75) }
## parallelise if on unix
if(.Platform$OS.type == "unix") {
    n.cores <- parallelly::availableCores(method = c("Slurm", "system"))-1
    foo <- parallel::mclapply(these.regions, my.g, mc.cores = n.cores)
} else {
    foo <- sapply(these.regions, my.g)
}
bar <- Reduce(my.f, foo)
plot(bar, col = rev(terrain.colors(101)), zlim = c(0, 100), main=this.country.name)
lines(get.border(this.country.code))
@ 

Now compare with a graph where the mean values are plotted for each region in the country, see figure \vref{fig:testing-means}. This figure is constructed with «spplot()« which is useful plotting monochrome regions, in contrast to the raw data which was a «raster« object.

<<testing-means, fig.cap = "Plotting the mean values of regions instead of mean values of pixels.">>=
temp.f <- function(i) {
    mean(raster::values(load.raster.from.disk(dhs.grid[i], deforestation.path = deforestation.path)), na.rm=TRUE)
}
if(.Platform$OS.type == "unix") {
    my.means <- unlist(parallel::mclapply(these.regions, temp.f, mc.cores = n.cores))
} else {
    my.means <- sapply(these.regions, temp.f)
}
this.country.code <- substring(names(dhs.grid)[1], 1, 3))
this.country.name <- globallivingconditions::iso.3166$String[match(this.country.code, globallivingconditions::iso.3166$numeric)]]
my.spdf <- sp::SpatialPolygonsDataFrame(dhs.grid[these.regions], data = data.frame(mean.coverage.in.year.2000 = my.means), match.ID = FALSE)
sp::spplot(my.spdf, col=NA, col.regions=rev(terrain.colors(101)), at = seq(0, 100, 1), scales = list(draw = TRUE), main = this.country.name, sp.layout = list(sf::as_Spatial(get.border(this.country.code)), col = "black"))
@

<<easy.ones>>=
temporal.loss.for.multiple.areas.from.a.single.grid.cell <- function(index, deforestation.path, ...) {
    ## if index is a vector, be smart, otherwise just call temporal.loss.directly.from.possible.multiple.files
    globallivingconditions::my.log.f(paste(Sys.time(), "temporal.loss.for.multiple.areas.from.a.single.grid.cell called with a vector argument", paste(index, collapse = ", ")), "deforestation.log")
    if(length(index) == 1){
        globallivingconditions::my.log.f(paste(Sys.time(), "Error: temporal.loss.for.multiple.areas.from.a.single.grid.cell called with a scalar argument", index), "deforestation.log")        
        return(NULL)
        ## return(temporal.loss.directly.from.possible.multiple.files(index, ...))
    }
    ## A single grid cells covers multiple areas.
    ## Verify that
    my.cover.files <- sapply(index, function(i) {
        file.name.from.bbox.f(bbox = sp::bbox(dhs.grid[i]), layer = "treecover2000")
    })
    if(length(table(my.cover.files)) > 1) {
        globallivingconditions::my.log.f(paste(Sys.time(), "Error: temporal.loss.for.multiple.areas.from.a.single.grid.cell called with a vector argument but not all cover files are identical", paste(index, collapse = ", ")), "deforestation.log")
        return(NULL)
    }
    my.loss.files <- file.name.from.bbox.f(bbox = sp::bbox(dhs.grid[index[1]]), layer = "lossyear")
    fetch.raster(c(my.cover.files, my.loss.files), deforestation.path = deforestation.path, ...)
    ## load the grid cells
    my.cover.raster <- raster::raster(paste0(deforestation.path, my.cover.files[1]))
    my.loss.raster <- raster::raster(paste0(deforestation.path, my.loss.files[1]))

    my.f <- function(i) {
        polygon <- dhs.grid[i]
        my.years = 1:21
        ## replicate stuff from temporal loss directly from single file
        loss <- raster::values(raster::mask(raster::crop(my.loss.raster, polygon), polygon))
        these.are.within.the.area <- which(is.na(loss) == FALSE)
        these.were.lost.indices <- which(loss > 0)
        size <- length(these.are.within.the.area)
        these.were.lost.values <- loss[these.were.lost.indices]
        rm(loss); trash <- gc(verbose = FALSE)        
        raster::removeTmpFiles(h=0)
        coverage <- raster::values(raster::mask(raster::crop(my.cover.raster, polygon), polygon))[these.are.within.the.area]
        raster::removeTmpFiles(h=0)
        average.coverage.at.t0 <- mean(coverage)    

        my.g <- function(year) {
            these.cells.lost.this.year.or.earlier <- these.were.lost.indices[which(these.were.lost.values <= year)]
            if(length(these.cells.lost.this.year.or.earlier) > 0){
                return(mean(c(coverage[-these.cells.lost.this.year.or.earlier],
                     rep(0, length(these.cells.lost.this.year.or.earlier))), na.rm=TRUE))
            } else {
                return(average.coverage.at.t0)
            }
        }
        
        average.coverage.per.year <- sapply(my.years, my.g)
        names(average.coverage.per.year) <- paste0("t", my.years)
        my.raster.stats <- list(size = size,
                coverage = c(t0 = average.coverage.at.t0, average.coverage.per.year)
                )
        absolute.loss.per.year <- sapply(1:(length(my.raster.stats$coverage)-1), function(i) { my.raster.stats$coverage[["t0"]] - my.raster.stats$coverage[[paste0("t", i)]] })
        globallivingconditions::my.log.f(paste(Sys.time(), "Ready with temporal loss for area", names(dhs.grid)[i]), "deforestation.log")    
        temporal.loss <- cbind(data.table::data.table(region = names(dhs.grid)[i],
                                                  size,
                                                  coverage.at.t0 = my.raster.stats$coverage[["t0"]]), t(data.frame(absolute.loss.per.year)))
    }
    results <- sapply(index, my.f, simplify = FALSE)
    rm(my.loss.raster, my.cover.raster); trash <- gc(verbose = FALSE)            
    raster::removeTmpFiles(h=0)
    return(results)
}

## For all regions, derive the necessary grid cells.
files.required <- sapply(1:length(dhs.grid), function(i) {
    file.name.from.bbox.f(bbox = sp::bbox(dhs.grid[i]), layer="treecover2000")
})

files.requiring.single.grid.cell <- which(sapply(files.required, length) == 1)
files.requiring.multiple.grid.cells <- which(sapply(files.required, length) > 1)

files.required.string <- sapply(1:length(dhs.grid), function(i) {
    paste(file.name.from.bbox.f(bbox = sp::bbox(dhs.grid[i]), layer="treecover2000"), collapse = ", ")
})

files.required.unique.string <- unique(files.required.string)
files.required.unique <- unique(files.required)
    

## Categorise regions based on the files the require.
categories.of.requirements <- sapply(unique(files.required.unique.string), function(required.files) {
    which(files.required.string == required.files)
})

single.tiff.file.area.requirements <- categories.of.requirements[which(sapply(files.required.unique, length) == 1)]
multiple.areas.depends.on.same.single.tiff <- single.tiff.file.area.requirements[which(sapply(single.tiff.file.area.requirements, length) > 1)]

## Start with the tricky ones, those that requires multiple tiff files.
remaining.ones <- (1:length(dhs.grid))[-unlist(multiple.areas.depends.on.same.single.tiff)]

## Split tasks by RAM requirements
area.sizes <- geosphere::areaPolygon(dhs.grid[remaining.ones])

n.cores <- parallelly::availableCores(method = c("Slurm", "system"))

## If RAM is plenty, then divide differently
## This if for a RAM restricted system
my.thresholds <- quantile(area.sizes, probs = c(0, 0.5, 0.7, 0.95, 0.99, 1.0))
parallelization.settings <- c(n.cores-1, floor(n.cores/2), max(floor(n.cores/4), 3), 2, 1)
areas.by.size.classes <- sapply(1:(length(my.thresholds)-1), function(i) {
    remaining.ones[which(area.sizes > my.thresholds[i] &
          area.sizes <= my.thresholds[i+1])] })

## If RAM is practically unrestricted, just order by size (largest first), make some arbitrary split and use all cores.
if(file.exists("/mimer/NOBACKUP/groups/globalpoverty1/hans")){
  my.thresholds <- quantile(area.sizes, probs = c(0, 0.5, 1.0))
  parallelization.settings <- rep(n.cores-1, times = length(my.thresholds) - 1)
  my.order <- order(area.sizes, decreasing = TRUE)
  the.largest.half <- remaining.ones[my.order[1:(length(area.sizes)/2-1)]]
  the.smallest.half <- remaining.ones[my.order[(length(area.sizes)/2):length(area.sizes)]]
  interleaved <- c(rbind(the.largest.half, rev(the.smallest.half)))
  areas.by.size.classes <- list(interleaved[1:(length(area.sizes)/2-1)],interleaved[(length(area.sizes)/2):length(area.sizes)])
}

average.coverage.over.time.directly.from.single.file <- function(polygon, my.cover.file, my.loss.file, deforestation.path = deforestation.path, ...){
    ## calculate size,  number of lost cells, per year.
    ## depends on raster.from.file.f()

    my.years = 1:21

    ## my.loss.raster <- raster.from.file.f(file = my.loss.file, my.mask = polygon, ...)
    my.loss.raster <- raster.from.file.f(file = my.loss.file, my.mask = polygon, deforestation.path = deforestation.path)
    loss <- raster::values(my.loss.raster)
    these.are.within.the.area <- which(is.na(loss) == FALSE)
    size <- length(these.are.within.the.area)
    if(size == 0){
        globallivingconditions::my.log.f(paste(Sys.time(), "found and an area of size 0, returning NULL"), "deforestation.log")    
        return(NULL)
    } else {
        globallivingconditions::my.log.f(paste(Sys.time(), "found and an area of size", size), "deforestation.log")        
    }
    these.were.lost.indices <- which(loss > 0)
    these.were.lost.values <- loss[these.were.lost.indices]
    rm(loss, my.loss.raster); trash <- gc(verbose = FALSE)
    raster::removeTmpFiles(h=0)

    ## my.cover.raster <- raster.from.file.f(file = my.cover.file, my.mask = polygon, ...)
    my.cover.raster <- raster.from.file.f(file = my.cover.file, my.mask = polygon, deforestation.path = deforestation.path)
    coverage <- raster::values(my.cover.raster)[these.are.within.the.area]
    rm(my.cover.raster); trash <- gc(verbose = FALSE)
    raster::removeTmpFiles(h=0)
    average.coverage.at.t0 <- mean(coverage)    
    ## The strategy is to calculate a new average for each year.
    ## And the final measure is the difference in these averages.

    my.f <- function(year) {
        these.cells.lost.this.year.or.earlier <- these.were.lost.indices[which(these.were.lost.values <= year)]
        ## calculate the new mean by adding zeros for all the lost cells, and removing the elements at those indices.
        if(length(these.cells.lost.this.year.or.earlier) > 0){
            ## since these.cells.lost.this.year.or.earlier are relative to these.were.lost,
            ## don't forget to include these.were.lost at next row.
            return(mean(c(coverage[-these.cells.lost.this.year.or.earlier],
                          rep(0, length(these.cells.lost.this.year.or.earlier))), na.rm=TRUE))
            ## <- na.rm=TRUE to handle pixels outside the region.
        } else {
            ## No loss has occured yet.
            return(average.coverage.at.t0)
        }
    }
    ## parallelise if on unix
    ## if(.Platform$OS.type == "unix") {    
    if(FALSE) {
        n.cores <- floor((parallelly::availableCores(method = c("Slurm", "system"))-1) * min(1, 1E8/size))
        globallivingconditions::my.log.f(paste(Sys.time(), "Using", n.cores, "for an area of size: ", size), "deforestation.log")    
        average.coverage.per.year <- parallel::mclapply(my.years, my.f, mc.cores = n.cores)
    } else {
        average.coverage.per.year <- sapply(my.years, my.f)
    }
    names(average.coverage.per.year) <- paste0("t", my.years)
    globallivingconditions::my.log.f(paste(Sys.time(), "Ready with temporal loss for an area of size", size, "for a polygon of size ",
                                           geosphere::areaPolygon(polygon)), "deforestation.log")    
    rm(polygon, my.cover.file, my.loss.file, coverage); gc(verbose = FALSE)
    raster::removeTmpFiles(h=0)
    return(list(size = size,
                coverage = c(t0 = average.coverage.at.t0, average.coverage.per.year)
                ))
}

temporal.loss.directly.from.possible.multiple.files <- function(index, deforestation.path = deforestation.path, ...){
    ## index refers to dhs.grid
    polygon <- dhs.grid[index]
    my.bbox <- sp::bbox(polygon)
    my.cover.files <- file.name.from.bbox.f(bbox = my.bbox, layer = "treecover2000")
    my.loss.files <- file.name.from.bbox.f(bbox = my.bbox, layer = "lossyear")    
    fetch.raster(c(my.cover.files, my.loss.files), deforestation.path = deforestation.path, ...)
    if(length(my.cover.files) > 1){
        my.raster.stats <- sapply(1:length(my.cover.files), function (i) {
            average.coverage.over.time.directly.from.single.file(polygon = polygon, my.cover.file = my.cover.files[i], my.loss.file = my.loss.files[i], deforestation.path = deforestation.path)
        }, simplify = FALSE)
        ## If any element is NULL remove it
        these.are.null <- which(sapply(my.raster.stats, is.null))
        if(length(these.are.null) > 0) {
            ## assume not all of them are null
            my.raster.stats <- my.raster.stats[-these.are.null]
        }
        ## if there are still more than one element, then create a weighted mean
        if(length(my.raster.stats) > 1){
            weights <- as.vector(prop.table(unlist(sapply(seq(from = 1, to = length(my.raster.stats)), function(i) { my.raster.stats[[i]][1] }))))
            ## restructure so that we don't need to know the number of tiff-files were used
            my.data <- Reduce(cbind, sapply(1:length(my.raster.stats), function(i) { my.raster.stats[[i]][[2]] }, simplify = FALSE))
            weighted.means <- sapply(1:nrow(my.data), function(i) {
                round(weighted.mean(x = unlist(my.data[i, ]), w = weights), 16)
            })
            names(weighted.means) <- paste0("t", 0:(length(weighted.means)-1))
            my.raster.stats <- list(size = sum(unlist(sapply(seq(from = 1, to = length(my.raster.stats)), function(i) { my.raster.stats[[i]][1] }))),
                                    coverage = weighted.means)
        } else {
            ## just flatten
            my.raster.stats <- my.raster.stats[[1]]
        }
    } else {
        my.raster.stats <- average.coverage.over.time.directly.from.single.file(polygon = polygon, my.cover.file = my.cover.files, my.loss.file = my.loss.files, ...)
    }
    absolute.loss.per.year <- sapply(1:(length(my.raster.stats$coverage)-1), function(i) { my.raster.stats$coverage[["t0"]] - my.raster.stats$coverage[[paste0("t", i)]] })
    temporal.loss <- cbind(data.table::data.table(region = names(dhs.grid)[index],
                                                  size = geosphere::areaPolygon(polygon),
                                                  coverage.at.t0 = my.raster.stats$coverage[["t0"]]), t(data.frame(absolute.loss.per.year)))
    globallivingconditions::my.log.f(paste(Sys.time(), "Ready with temporal loss for region", names(dhs.grid)[index]), "deforestation.log")
    return(temporal.loss)
}

my.means <- data.table::rbindlist(sapply(1:length(areas.by.size.classes), function(i){
    globallivingconditions::my.log.f(paste(Sys.time(), "Starting processing files in size class", i), "deforestation.log")    
    cl <- snow::makeCluster(parallelization.settings[i], type = "SOCK")
    doParallel::registerDoParallel(cl)
    snow::clusterExport(cl, c("base.url", "dhs.grid", "deforestation.path", "raster.from.file.f", "point.at.corners.f", "url.from.point.f", "url.from.file.f", "file.name.from.bbox.f",
                              "fetch.raster", "average.coverage.over.time.directly.from.single.file", "temporal.loss.directly.from.possible.multiple.files"))
    results <- data.table::rbindlist(foreach::foreach(
                        j=areas.by.size.classes[[i]],
                        .packages=c("sp")
                        ) %dopar% {
                            my.data.table <- try(temporal.loss.directly.from.possible.multiple.files(j, deforestation.path = deforestation.path), silent = TRUE)
                            if(length(attr(my.data.table, which = "condition")) > 0){
                                globallivingconditions::my.log.f(paste(Sys.time(), "temporal.loss.directly.from.possible... failed on element", j, "in class", i), "deforestation.log")
                                return(NULL)
                            }
                            return(my.data.table)
                        })
    stopCluster(cl)
    globallivingconditions::my.log.f(paste(Sys.time(), "Finished processing files in size class", i, "about to return an object of class:", class(results)[1]), "deforestation.log")    
    return(results)
  }, simplify = FALSE))

## The easy ones - areas that share a single grid cell.
cl <- snow::makeCluster(parallelly::availableCores(method = c("Slurm", "system")), type = "SOCK")
library(doParallel)
doParallel::registerDoParallel(cl)
snow::clusterExport(cl, c("base.url", "dhs.grid", "deforestation.path", "raster.from.file.f", "point.at.corners.f", "url.from.point.f", "url.from.file.f", "file.name.from.bbox.f",
                          "fetch.raster", "average.coverage.over.time.directly.from.single.file", "temporal.loss.directly.from.possible.multiple.files"))
easy.ones <- data.table::rbindlist(foreach::foreach(
            j=1:length(multiple.areas.depends.on.same.single.tiff),
            .packages=c("sp"),
            .inorder=FALSE
            ) %dopar% {
           my.data.table <- try(data.table::rbindlist(temporal.loss.for.multiple.areas.from.a.single.grid.cell(multiple.areas.depends.on.same.single.tiff[[j]], deforestation.path = deforestation.path)), silent = TRUE)
           if(length(attr(my.data.table, which = "condition")) > 0){
               globallivingconditions::my.log.f(paste(Sys.time(), "temporal.loss.for.multiple.areas.from.a.single.grid.cell... failed on element", j), "deforestation.log")
               return(NULL)
           }
           return(my.data.table)
                })
stopCluster(cl)

@ 

Before merging the results, do some imputations.

Angola (2015 + 2011 (MIS) 2006-7 (MIS), Madagascar (2021, 2008-2009, 2003-2004 (no gis), 1997), South Africa (1998, 2016), Burundi (2010, 2016-17), Congo (2005, 2011-12), Congo Democratic Republic (2007, 2013-14), Gabon (2000, 2012, 2019-2021), Cote d'Ivoire (2021, 2011-12, 2005), Chad (2004 (no gis), 2014-15), Tanzania (1999, 2004-5, 2010, 2015-16, 2022), Morocco (not enough data), Egypt (2000, 2005, 2008, 2014).

Merge the results.

<<merge.results>>=
library(data.table)
temporal.loss <- rbind(easy.ones, my.means)
names(temporal.loss)[match("region", names(temporal.loss))] <- "superClusterID"
my.dt <- get(load(file = "living-conditions.RData"))
data.table::setkey(my.dt, superClusterID)
data.table::setkey(temporal.loss, superClusterID)
my.dt.with.temporal.loss <- merge(my.dt, temporal.loss)

## add forest class
my.dt.with.temporal.loss[, forest.class := cut(coverage.at.t0, 
                               breaks = c(0, 5, 10, 20, 40, 100),
                               include.lowest = TRUE), ]

## add loss at year of interview
my.dt.with.temporal.loss$loss.at.year.of.interview = NA
for(t in 1:21){
    these.cases <- which(my.dt.with.temporal.loss$year.of.interview == 2000 + t)
    this.column.name <- paste0("V", t)
    foo <- my.dt.with.temporal.loss[these.cases, this.column.name, with = FALSE]
    my.dt.with.temporal.loss$loss.at.year.of.interview[these.cases] <- foo[[this.column.name]]
}


dep.subset.f <- function(var.name){
    ## To control for the sampling noise, include the average education in year for the persons that were between 24 and 44 years old in year 2000
    ## This group is stable through the study period (at the end the oldes are 65 years old, still included if alive)              
    ## We here make use of the fact the education is a stable feature in the sense it can not be lost. It can be improved, but that should be so uncommon that it should not affect the analyses at all.
    ## By including this average, the model can ascribe some part of the variation in the outcome over time in the same area to variation in the educational average, which means that the persons surveyed differed, and reduce the 
    ## problem of sampling noise.

    ## For every gadm area and year of interview, select the persons in the right age interval (24-44 in 2000), so
    ## youngest = year.of.interview - 2000 + 24
    ## oldest = year.of.interview - 2000 + 444

    my.dt.with.temporal.loss$dep.var <- my.dt.with.temporal.loss[[var.name]]

    foo <- my.dt.with.temporal.loss[, list(
        year.of.interview,
	age,
	education.in.years,
        superClusterID,
        source), ]
    bar <- foo[age >= year.of.interview - 2000 + 24 & age <= year.of.interview - 2000 + 44, ]
    ## For every gadm.area and source, calculate the average years of education.
    educational.data <- bar[, list(average.years.of.education.for.the.1956.1976.cohort = mean(education.in.years, na.rm = TRUE)), by = list(superClusterID, source)]

    ## select cases with non-missing value on the deprivation after calculating the educational data, since food deprivation is only measured on children below 5 and education is only calculate for adults above 23.
    these <- which(is.na(my.dt.with.temporal.loss[[var.name]]) == FALSE &
                   is.na(my.dt.with.temporal.loss$loss.at.year.of.interview) == FALSE
                   )

    ## if data collection happened over two different years, then there are two separate lines.
    ## merge these and take the mean year.


    output.dt <- my.dt.with.temporal.loss[these, list(
        deprived = sum(length(which(dep.var))),
        raw.year = mean(year.of.interview),
        loss.at.year.of.interview = mean(loss.at.year.of.interview),
        not.deprived = sum(.N) - sum(length(which(dep.var))),
        country = unique(globallivingconditions::iso.3166$String[match(country.code.ISO.3166.alpha.3, globallivingconditions::iso.3166$numeric)]),
        forest.class = unique(forest.class)), by = list(superClusterID, source)]
    
    output.dt[, year := scale(raw.year), ]
    setkey(output.dt, country, raw.year)
    output.dt.1 <- merge(output.dt, country.data)
    output.dt.1$country <- factor(output.dt.1$country)
    output.dt.1[, GDP.z := scale(GDP), ]

    setkey(output.dt.1, superClusterID, source)
    setkey(educational.data, superClusterID, source)    
    output.dt.2 <- merge(output.dt.1, educational.data)

    output.dt.2[, average.years.of.education.for.the.1956.1976.raw := average.years.of.education.for.the.1956.1976.cohort, ]    
    output.dt.2[, average.years.of.education.for.the.1956.1976.cohort := scale(average.years.of.education.for.the.1956.1976.cohort), ]

    output.dt.2$superClusterID <- factor(output.dt.2$superClusterID)

    return(output.dt.2)
}
my.deps <- c("severe.shelter.deprivation", "severe.water.deprivation", "severe.food.deprivation", "severe.sanitation.deprivation")
data.sets <- sapply(my.deps, dep.subset.f, simplify = FALSE)
save(my.dt.with.temporal.loss, file = "my.dt.with.temporal.loss.RData")
rm(my.dt, my.dt.with.temporal.loss, easy.ones, my.means)
gc(verbose=FALSE)

@

\section{Shelter deprivation}
Create a data set suitable for regression with shelter deprivation as the dependent variable.


<<continued.for.the.extract, echo = TRUE>>=
test.dt <- data.sets[[match("severe.shelter.deprivation", my.deps)]]
@

\subsection{Coverage for shelter}
Show the areas for which we have complete data for shelter deprivation, and mark areas where education is missing and where there are too few measurements.

<<coverage-shelter, fig.cap = "Coverage for education in the shelter deprivation dataset. Green = multiple measurements available, Red = a single measurement available, White = no measurement available.", dev = "png", dpi = 200>>=
plot(africa)
missing.stats <- test.dt[is.na(average.years.of.education.for.the.1956.1976.cohort) == FALSE, length(unique(raw.year)), by = superClusterID]
these <- missing.stats[V1 > 1, match(superClusterID, names(dhs.grid))]
plot(dhs.grid[these], col = "green", add = TRUE)
these <- missing.stats[V1 == 1, match(superClusterID, names(dhs.grid))]
plot(dhs.grid[these], col = "red", add = TRUE)
my.borders <- do.call(rbind, lapply(country.codes, get.border))
plot(my.borders, col = NA, border = "red", add = TRUE)
@ 

The basic unit in the data set is a rather small administrative area, on average there are \Sexpr{round(length(dhs.grid) / length(country.codes))} such areas in each country in the data set. For each DHS wave, area and type of deprivation, we aggregate the number of deprived persons, and the total number of surveyed persons, which in most areas gives repeated measurements. The data on deforestation and the country level variables are available for every year, and is merged to a single dataset. In addition we also use the larger top level regions in each country, and the whole country to get two baselines for the change in deprivation of time. These baselines are useful in that they make sure that the signal picked up by the regression terms for deforestation is uncorrelated to trends that are shared by local areas that differ in level of deforestation.

Table \vref{tab:data} provides an excerpt from the data set that includes data on severe shelter deprivations, containing \Sexpr{nrow(test.dt)} data points about \Sexpr{length(unique(test.dt$superClusterID))} unique areas.

<<continued.excerpt, echo = FALSE, results = "asis">>=
library(data.table)
foo <- test.dt[, c("loss.at.year.of.interview", 
"GDP", "forest.class", "year", "superClusterID", "average.years.of.education.for.the.1956.1976.raw", "deprived", "not.deprived")]
## setDT(foo)
setkey(foo, superClusterID, year)
colnames(foo) <- c("DF", "GDP", "FC", "year", "area", "EduY", "D.", "Non D.")
foo[, year := year * attr(year, "scaled:scale") + attr(year, "scaled:center"), ]
library(xtable)
print(xtable(foo[area == "288.2.Ashanti.Adansi North", ], digits = c(0, rep(2, 2), rep(0, 3), 2, rep(0, 2)), label = "tab:data", caption = "Abbreviations: Deforestation ('DF'), Forest Class ('FC'), Average years of education for the 1956 1976 cohort ('EduY') Deprived ('D.'), and Not.Deprived ('Non D.') for an area in Ghana."), include.rownames = FALSE, booktabs = TRUE)
@ 


\end{document}

